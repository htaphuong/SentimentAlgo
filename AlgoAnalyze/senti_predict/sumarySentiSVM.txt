---------------------------------------------------
10.train	10.test
---------------------------------------------------
Loading unigram model...OK
Creating lexical segmenter...
Load the lexicon automaton... OK.
External lexicon loaded.
Lexical segmenter created.
Initializing tokenizer...OK
.*
optimization finished, #iter = 978
nu = 0.6412560501946929
obj = -22991.817220550198, rho = 0.5034254654946085
nSV = 564, nBSV = 224
.*
optimization finished, #iter = 1075
nu = 0.4507145281275975
obj = -18004.893921221803, rho = 1.2821918402609207
nSV = 576, nBSV = 148
.*
optimization finished, #iter = 1100
nu = 0.5064864129642443
obj = -19658.195265956132, rho = -2.6827787024583105
nSV = 581, nBSV = 156
Total nSV = 887
Accuracy = 63.0% (63/100) (classification)

